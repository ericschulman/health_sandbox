{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0501d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from functools import partial, reduce\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e5a69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2017'\n",
    "data_out = '../data/processed_data/'\n",
    "data_in = '../data/data_' + year + '/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9885911e",
   "metadata": {},
   "source": [
    "## Preprocess numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eefe4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ServiceAreaId', 'IssuerId', 'TEHBOutOfNetFamilyPerPersonMOOP',\n",
      "       'TEHBOutOfNetFamilyPerGroupMOOP', 'SBCHavingDiabetesCoinsurance',\n",
      "       'SBCHavingaBabyCopayment', 'EHBPercentTotalPremium',\n",
      "       'TEHBInnTier1IndividualMOOP', 'SBCHavingDiabetesCopayment', 'act_value',\n",
      "       'TEHBInnTier1FamilyPerGroupMOOP', 'TEHBOutOfNetIndividualMOOP',\n",
      "       'SBCHavingDiabetesDeductible', 'TEHBCombInnOonFamilyPerGroupMOOP',\n",
      "       'TEHBInnTier1FamilyPerPersonMOOP', 'SBCHavingaBabyCoinsurance',\n",
      "       'SBCHavingaBabyDeductible', 'OutOfServiceAreaCoverageDescription',\n",
      "       'TEHBCombInnOonIndividualMOOP', 'TEHBCombInnOonFamilyPerPersonMOOP'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "column_info = pd.read_csv(data_out +'plan_summary.csv')\n",
    "plan_data = pd.read_csv(data_in + 'Plan_Attributes_PUF_' + year + '.csv',encoding='cp1252')\n",
    "\n",
    "act_value = plan_data[['IssuerActuarialValue','AVCalculatorOutputNumber']].copy()\n",
    "act_value =  act_value.fillna(0)\n",
    "act_value['IssuerActuarialValue'] = act_value['IssuerActuarialValue'].apply( lambda x: float(str(x).replace('%','')) )\n",
    "act_value['AVCalculatorOutputNumber'] = act_value['AVCalculatorOutputNumber']*100\n",
    "act_value['act_value'] = act_value.max(axis=1)\n",
    "\n",
    "plan_data['act_value'] = act_value['act_value']\n",
    "\n",
    "good_columns = column_info['Column_Name'][ (column_info['Missing_Values'] <= 3000) & (column_info['Missing_Values'] >= 0) & (column_info['Unique_Values'] >= 90) \n",
    "                  & (column_info['Unique_Values'] <= 370) ]\n",
    "good_columns = list(good_columns) + ['act_value']\n",
    "\n",
    "#fix the set of columns to remove bad ones\n",
    "bad_columns = ['SBCHavingSimplefractureCoinsurance', \n",
    " 'SBCHavingSimplefractureCopayment', 'SBCHavingSimplefractureDeductible', 'ServiceAreaID']\n",
    "good_columns = list(set(good_columns) - set(bad_columns))\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/4703390/how-to-extract-a-floating-number-from-a-string\n",
    "clean_columns = plan_data[['ServiceAreaId','IssuerId']].copy()\n",
    "\n",
    "for column in good_columns:\n",
    "    #try to extract the text\n",
    "    test_column = plan_data[column].copy()\n",
    "    test_column = test_column.fillna(0)\n",
    "    \n",
    "    #if its just an int go with that\n",
    "    int_column = test_column.apply(lambda x : float( (re.findall(r'^-?\\d+(?:\\.\\d+)$',str(x)) +['0']) [0]) )\n",
    "    if int_column.nunique() >= 3:\n",
    "        clean_columns[column] = int_column\n",
    "        \n",
    "    #otherwise try something else\n",
    "    dollar_column = test_column.apply(lambda x : int((re.findall('\\\\$(\\\\d+)',str(x) ) +['0'])[0]) )\n",
    "    if dollar_column.nunique() >= 3:\n",
    "        clean_columns[column] = dollar_column\n",
    "\n",
    "print(clean_columns.columns)\n",
    "\n",
    "#IssuerActuarialValue, AVCalculatorOutputNumber\n",
    "\n",
    "clean_columns.describe()\n",
    "clean_columns.to_csv(data_out  + 'issuer_numeric_' +year +'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68647014",
   "metadata": {},
   "source": [
    "## Issuer characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b51414a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21365, 129)\n",
      "(18556, 129)\n",
      "7269\n",
      "6305\n",
      "3126\n",
      "3008\n"
     ]
    }
   ],
   "source": [
    "plan_df = pd.read_csv(data_in + 'Plan_Attributes_PUF_' + year + '.csv',encoding='cp1252')\n",
    "plan_summary = pd.read_csv( data_out + 'plan_summary.csv', index_col=None)\n",
    "\n",
    "\n",
    "#determine which values are missing\n",
    "no_missing_values = plan_summary[(plan_summary['Missing_Values'] == 0) & \n",
    "                                 (plan_summary['Unique_Values'] <= 34)]['Column_Name'].to_list()\n",
    "no_missing_values.extend(['IssuerId','ServiceAreaId','StandardComponentId','StateCode'])\n",
    "no_missing_values.remove('PlanEffectiveDate')\n",
    "no_missing_values.remove('DesignType')\n",
    "\n",
    "\n",
    "# Treat columns as continous\n",
    "cleaned_plan_df = plan_df[no_missing_values]\n",
    "continuous = ['FirstTierUtilization','BeginPrimaryCareCostSharingAfterNumberOfVisits',\n",
    "              'BeginPrimaryCareDeductibleCoinsuranceAfterNumberOfCopays']\n",
    "cleaned_plan_df['FirstTierUtilization'] = cleaned_plan_df['FirstTierUtilization'].str.replace('%','')\n",
    "cleaned_plan_df[continuous] = cleaned_plan_df[continuous].astype(str).astype(float) \n",
    "\n",
    "# Get CSRVariationType binary\n",
    "def largest_dummies(cleaned_plan_df, col_name,number_dummies):\n",
    "    categories = cleaned_plan_df[col_name].value_counts()\n",
    "    largest = categories.head(number_dummies).index.to_list()\n",
    "    rest = categories[number_dummies:].to_list()\n",
    "    cleaned_plan_df[col_name + 'Other'] = 1\n",
    "    for j in range(len(largest)):\n",
    "        category_name = largest[j]\n",
    "        cleaned_plan_df.loc[ cleaned_plan_df[col_name] == largest[j], col_name+str(category_name)] = 1\n",
    "        cleaned_plan_df.loc[ cleaned_plan_df[col_name] != largest[j], col_name+str(category_name)] = 0\n",
    "        cleaned_plan_df.loc[ cleaned_plan_df[col_name] == largest[j], col_name + 'Other'] = 0\n",
    "    return cleaned_plan_df\n",
    "\n",
    "cleaned_plan_df = largest_dummies(cleaned_plan_df, 'CSRVariationType',4)\n",
    "cleaned_plan_df = largest_dummies(cleaned_plan_df, 'IssuerId',10)\n",
    "cleaned_plan_df = largest_dummies(cleaned_plan_df, 'StateCode',50)\n",
    "\n",
    "dummy_cols = ['BusinessYear',\n",
    " 'SourceName',\n",
    " 'MarketCoverage',\n",
    " 'DentalOnlyPlan',\n",
    " 'IsNewPlan',\n",
    " 'PlanType',\n",
    " 'MetalLevel',\n",
    " 'QHPNonQHPTypeId',\n",
    " 'CompositeRatingOffered',\n",
    " 'ChildOnlyOffering',\n",
    " 'OutOfCountryCoverage',\n",
    " 'OutOfServiceAreaCoverage',\n",
    " 'NationalNetwork',\n",
    " 'MultipleInNetworkTiers',\n",
    " 'InpatientCopaymentMaximumDays']\n",
    "cleaned_plan_df = pd.get_dummies(cleaned_plan_df, columns = dummy_cols)\n",
    "cleaned_plan_df\n",
    "\n",
    "issuer_numeric = pd.read_csv(data_out + 'issuer_numeric_' + year + '.csv')\n",
    "issuer_numeric = issuer_numeric.iloc[: , 1:]\n",
    "merged_df = cleaned_plan_df.merge(issuer_numeric, left_index=True,right_index=True,suffixes=('', '_y'))\n",
    "\n",
    "#clean up the columns \n",
    "relevant_cols = list(merged_df.columns)\n",
    "for word in ['IssuerId','ServiceAreaId','IssuerId_y','ServiceAreaId_y']:\n",
    "    relevant_cols.remove(word)\n",
    "relevant_cols = ['IssuerId','ServiceAreaId','StandardComponentId'] + relevant_cols \n",
    "merged_df = merged_df[relevant_cols]\n",
    "merged_df\n",
    "\n",
    "#drop the dental plan data?\n",
    "print(merged_df.shape)\n",
    "merged_df  = merged_df[plan_df['DentalOnlyPlan']=='No']\n",
    "print(merged_df.shape)\n",
    "\n",
    "merged_df.to_csv(data_out + 'issuer_characteristics_'+ year + '.csv', index=False)\n",
    "\n",
    "\n",
    "## Adding the HIX data\n",
    "#add in the hix data\n",
    "cms_data = pd.read_csv( data_out + 'issuer_characteristics_'+ year + '.csv')\n",
    "#clean up planids in the HIX data...\n",
    "hix_data = pd.read_csv(data_in + 'plans_'+ year + '.csv')\n",
    "hix_data['PLANID2'] = hix_data['PLANID'] #+ new_plan_df['AREA'].apply(lambda x : '-' + x[-2:])\n",
    "hix_data['PLANID2'] = hix_data['PLANID2'].apply(lambda x : x[:-3])\n",
    "no_dash = hix_data['PLANID'].apply(lambda x : x.find('-') == -1 )\n",
    "hix_data['PLANID2'][no_dash] = hix_data['PLANID'][no_dash]\n",
    "\n",
    "hix_data = hix_data[['PLANID2','PREMIC', 'PREMI27', 'PREMI50', 'PREMI2C30', 'PREMC2C30']]\n",
    "hix_data = hix_data.fillna(0)\n",
    "\n",
    "#take the median for a plan... unclear what area means...\n",
    "hix_data = hix_data.groupby('PLANID2',as_index=False).median()\n",
    "\n",
    "print(hix_data['PLANID2'].nunique())\n",
    "print(hix_data['PREMI50'].nunique())\n",
    "\n",
    "#first merge on ones where we know the service area?\n",
    "merged_df = cms_data.merge(hix_data,how='left', left_on=['StandardComponentId'], \n",
    "                              right_on=['PLANID2'])\n",
    "merged_df = merged_df.fillna(0)\n",
    "\n",
    "print(merged_df['PLANID2'].nunique())\n",
    "print(merged_df['PREMI50'].nunique())\n",
    "\n",
    "merged_df = merged_df.drop(labels=['PLANID2'],axis=1)\n",
    "merged_df.to_csv(data_out + 'issuer_characteristics_' + year + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6407b4d3",
   "metadata": {},
   "source": [
    "## County Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c1094a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3220, 139)\n",
      "(3220, 90)\n",
      "(3220, 62)\n"
     ]
    }
   ],
   "source": [
    "acs1 = pd.read_csv(data_in + 'ACSDP5Y' + year + '.DP03_data_with_overlays.csv', na_values=['(X)']) #, encoding='cp1252'\n",
    "acs2 = pd.read_csv(data_in + 'ACSDP5Y' + year + '.DP05_data_with_overlays.csv', na_values=['(X)']) \n",
    "acs3 = pd.read_csv(data_in + 'ACSST5Y' + year + '.S1701_data_with_overlays.csv', na_values=['(X)'])\n",
    "\n",
    "#DP03_0095E - health care coverage\n",
    "\n",
    "def remove_cols(df):\n",
    "    \n",
    "    #save county\n",
    "    county = df['GEO_ID']\n",
    "    county = county.str[-5:]\n",
    "    county = county.astype(str) \n",
    "    county = county.loc[1:]\n",
    "    \n",
    "    #find the bad columns\n",
    "    cols = list(df.columns)\n",
    "    trim_cols1 = []\n",
    "    for col in cols:\n",
    "        income_col = 'C01_' in col and 'E' in col\n",
    "        if 'PE' in col or 'DP03_0095E' in col or income_col:\n",
    "            trim_cols1.append(col)\n",
    "\n",
    "    df = df[trim_cols1]\n",
    "    df = df.loc[1:]\n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    \n",
    "    #filter out bad columns\n",
    "    df['County'] = county\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for i in [acs1,acs2,acs3]:\n",
    "    dfs.append(remove_cols(i))\n",
    "\n",
    "for i in dfs:\n",
    "    print(i.shape)\n",
    "\n",
    "\n",
    "[acs1, acs2,acs3] = dfs\n",
    "merge = partial(pd.merge, on=['County'], how='outer')\n",
    "all_acs = reduce(merge, dfs)\n",
    "all_acs.to_csv(data_out + 'county_characteristics_' + year + '.csv', index=False)\n",
    "\n",
    "\n",
    "# Comment this section of the code for 2017 data - START\n",
    "if year == '2016':\n",
    "    csr = pd.read_csv(data_in + 'csrzipcounty2016.csv', na_values=['.'])\n",
    "    csr.columns = csr.iloc[2]\n",
    "    csr = csr.iloc[4:-1]\n",
    "    csr.reset_index(drop=True)\n",
    "    csr\n",
    "\n",
    "    csr_clean = csr.copy()\n",
    "    csr_clean = csr_clean\n",
    "    for column in list(csr.columns)[3:]:\n",
    "        csr_clean[column] = csr_clean[column].fillna('0')\n",
    "        csr_clean[column] = csr_clean[column].apply(lambda x: float((str(x)).replace('$','').replace(',','')) )\n",
    "        \n",
    "    merged_acs = all_acs.merge(csr_clean, how='left', left_on='County', right_on='FIPS County Code')\n",
    "    merged_acs.fillna(0, inplace = True)\n",
    "    merged_acs.to_csv(data_out + 'county_characteristics_' + year + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151bdf35",
   "metadata": {},
   "source": [
    "## Merge characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b064411",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4537, 132)\n",
      "(5698, 135)\n"
     ]
    }
   ],
   "source": [
    "enroll_df = pd.read_csv(data_in + year + '-Issuer-Enrollment-Disenrollment-Report.csv',na_values=['*'])\n",
    "enroll_df = enroll_df[['HIOS ID','Policy County FIPS Code','Ever Enrolled Count']].fillna('0')\n",
    "#clean the column\n",
    "enroll_df['Ever Enrolled Count'] = enroll_df['Ever Enrolled Count'].apply(lambda x: float(str(x).replace(',','')) )\n",
    "\n",
    "service_df = pd.read_csv(data_in + 'ServiceArea_PUF_'  + year + '.csv', encoding='cp1252')\n",
    "service_df = service_df[['County','ServiceAreaId','IssuerId']]\n",
    "service_df = service_df.drop_duplicates()\n",
    "issuer_df = pd.read_csv(data_out + 'issuer_characteristics_' +year +'.csv')\n",
    "\n",
    "issuer_service = issuer_df.merge(service_df, how='inner', on=['ServiceAreaId', 'IssuerId'])\n",
    "issuer_service = issuer_service[ ~issuer_service['County'].isna() ]\n",
    "\n",
    "#reorder the columns, drop service area\n",
    "col_order= ['IssuerId','County'] + list(issuer_service.columns)[2:-1]\n",
    "issuer_service = issuer_service[ col_order ]\n",
    "\n",
    "#group by county\n",
    "#pre_cols = list(issuer_service.columns)\n",
    "issuer_service = issuer_service.groupby(['IssuerId','County','StandardComponentId','StateCode'],as_index=False).median()\n",
    "issuer_service_count = issuer_service.groupby(['IssuerId','County','StateCode'],as_index=False)['StandardComponentId'].count()\n",
    "issuer_service = issuer_service.groupby(['IssuerId','County','StateCode'],as_index=False).mean()\n",
    "issuer_service['Plan Counts'] = issuer_service_count['StandardComponentId']\n",
    "issuer_service = issuer_service.rename(columns={'StateCode':'State'})\n",
    "#post_cols = list(issuer_service.columns)\n",
    "print(issuer_service.shape)\n",
    "#print(len(pre_cols),len(enroll_df))\n",
    "\n",
    "#fix issue with merging on float\n",
    "enroll_df['Policy County FIPS Code'] = enroll_df['Policy County FIPS Code'].apply( lambda x : (int(x)) )\n",
    "enroll_df['HIOS ID'] = enroll_df['HIOS ID'].apply( lambda x : (int(x)) )\n",
    "issuer_service['County'] = issuer_service['County'].apply( lambda x : (int(x)) )\n",
    "issuer_service['IssuerId'] = issuer_service['IssuerId'].apply( lambda x : (int(x)) )\n",
    "\n",
    "enroll_issuer = enroll_df.merge(issuer_service, how='left', right_on=['County', 'IssuerId'],\n",
    "                                     left_on=['Policy County FIPS Code','HIOS ID'])\n",
    "# enroll_issuer.to_csv('test3.csv')\n",
    "enroll_issuer['County'] = enroll_issuer['Policy County FIPS Code']\n",
    "enroll_issuer['IssuerId'] = enroll_issuer['HIOS ID']\n",
    "enroll_issuer = enroll_issuer.fillna(0)\n",
    "print(enroll_issuer.shape)\n",
    "\n",
    "\n",
    "county = pd.read_csv(data_out + 'county_characteristics_' + year + '.csv')\n",
    "result = enroll_issuer.merge(county, how='left', left_on='County', right_on='County')\n",
    "result.fillna(0,inplace=True)\n",
    "result = result[result['DP03_0095E'] > 0] # only plans with enrollees\n",
    "result =result[~result['Policy County FIPS Code'].isna()]\n",
    "\n",
    "\n",
    "#weird thing - need to remove state?\n",
    "if year == '2016':\n",
    "    result = result.rename(columns={'State_x':'State'})\n",
    "\n",
    "# process columns\n",
    "all_cols  = list(result.columns)\n",
    "keys = ['HIOS ID','Policy County FIPS Code','IssuerId','County','State']\n",
    "delete_key = ['State_y', 'FIPS County Code', 'County Name']\n",
    "\n",
    "for key in keys:\n",
    "    all_cols.remove(key)\n",
    "\n",
    "if year == '2016':\n",
    "    for key in delete_key: \n",
    "        all_cols.remove(key)\n",
    "    \n",
    "#delete bad columns/clean up census data\n",
    "all_cols2 = []\n",
    "\n",
    "for col in all_cols:\n",
    "    #fix cols from census data\n",
    "    result[col] = result[col].apply(lambda x : float(str(x).replace('-','0').replace('N','0')))\n",
    "    \n",
    "    #clean up cols with no variance  \n",
    "    if result[col].std() > 0:\n",
    "        all_cols2.append(col)  \n",
    "\n",
    "result[keys + all_cols2].to_csv(data_out + 'merged_characteristics_' + year + '.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d428157f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in first list: ['DP05_0032PE', 'DP05_0004PE', 'OutOfCountryCoverage_YES', 'IssuerId56503', 'Average Monthly Advanced CSR Payment for Consumers with 87%', 'ChildOnlyOffering_Allows Child-Only', 'OutOfServiceAreaCoverage_YES', 'Number of Consumers with CSR (AV of 73%/87%/94%)', 'Number of Consumers with CSR AV of 87%', 'DP05_0018PE', 'IssuerId84670', 'BusinessYear_2016', 'Number of Consumers with CSR AV of 94%', 'DP05_0028PE', 'IssuerId14002', 'Average Monthly Advanced CSR Payment for Consumers with 94%', 'StateCodeSC', 'Average Monthly Advanced CSR Payment for Consumers with 73%', 'StateCodeAL', 'Number of Consumers with CSR AV of 73%', 'InpatientCopaymentMaximumDays_1', 'PlanType_Indemnity', 'IssuerId68781', 'PREMIC', 'IssuerId27357', 'NationalNetwork_NO', 'Total Number of Consumers', 'InpatientCopaymentMaximumDays_2']\n"
     ]
    }
   ],
   "source": [
    "if year != '2016':\n",
    "    merged_characteristics_2017 = pd.read_csv(data_out + 'merged_characteristics_' + year + '.csv')\n",
    "    list2 = list(merged_characteristics_2017.columns)\n",
    "    #use 2016 as a benchmark\n",
    "    merged_characteristics = pd.read_csv(data_out + 'merged_characteristics_2016.csv')\n",
    "    list1 = list(merged_characteristics.columns)\n",
    "    list_missing = list(set(list1).difference(list2))\n",
    "    print(\"Missing values in first list:\", list_missing)\n",
    "\n",
    "    for col in list_missing:\n",
    "        merged_characteristics_2017[col] = 0\n",
    "        \n",
    "    merged_characteristics_2017.to_csv(data_out + 'merged_characteristics_' + year + '.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
